
--- Turn Any Website into a Chatbot ï½œ n8n + Firecrawl Tutorial.txt ---
 In this video I will show you how to scrape any websites and turn it into a chatbot. We use NAN and also FireCrawl. Here is a Creek demo. I have already scraped data from hacker news and I want to know about the latest hacker news and explain it to me simply. So I will say what's new hacker news and explain it simply. So first we see there is an Nvidia programming language shift and there's also this fin2 update although I don't know what's fined so you can ask you know what is finned. You can see that you can ask questions about it and if you find something interesting you can even suggest LinkedIn posts on this topic. Although you write a pretty standard AI social post but you get that idea you you can have this as a base to work off and and have different ideas that come to mind and because this is scraping data from websites that update data often that you can always have new ideas that way. To do this you have to set up a workflow that first scraped the data from the website and then store the data into a vector database. In this example I'm using Pineco and for scraping I'm using this thing called fire crawl and you can just go to firecrawl.dev and for you to just try it out you can go to playground and go to extract I think that's the coolest functionality then you can describe in plain English what you want to scrape from this page and for me the first step is to extract the top 10 posts from hacker news and I only want the post title and also the post link and you can see that we can go into the workflow that adds the vector data and right now I just have it as a manual trigger where I just click on test workflow and run through the whole workflow and then the first step is that I call this extract endpoint which is the same thing you saw in the playground. It will extract the data and base on the prompt and then the prompt is in here. You pass it along as a body within the post request and you have the URL here and in the future you could have it a dynamic URL where you just ask it to scrape a website and it does it for you but that will be a little bit more complicated. Right now I'm just trying to do with one website which is hacker news and other parameters you can just get it from the documents and one thing that's very critical is the schema because first it wasn't very clear on the documentation the schema I couldn't find what you should put in there but if you go to playground and then you already try to do the same extract methods and you can click on this JSON view where you can see schema and then you want to copy in this whole thing into the schema key. So you see like from here you want to copy in all those data I guess it's easier to see if you enlarge it. You can see it in schema all this is coming from the playground example. The other things I just have as force you could enable depending on your needs. So that's the first step. After you have that done you notice that if you run this link it doesn't return your resort because the extract methods from firecrot takes some time to complete so they actually separate out the method to get the result of the extraction. So if you go to the documentation and go to API reference then you go under extract endpoints. So extract is the one we just used and it tells you where you should put in all that stuff and then I already go through the schema stuff and also the authorization is just the bear plus your API key. You just need to add it in your header auth in NAN and to get the result you want to go to get extract status and that's where you call extract and then you pass in the ID where you got from from the previous step. So you can see that it returns the ID from this extract endpoint. You just pass that in then you should be able to get the the result and see in here. I'm passing the ID but before that I'm actually just checking if the API access is fully completed. If it completed I went to wait for 10 seconds because the extraction going to take some time anyway so I'm going to wait for 10 seconds and then call this API to get the result and let's see if we can run. And you can see that the ID is there and let's see test step and you can see that we get the results in here now. We have the link and then the title and like we just saw in the playground. So that's how you replicate the same result. I just have all this just to make sure that it tried keep trying to get the result if the status is processing. So like the the API will return a thing called status and then if it's processing then I I wafered in other 10 seconds and I called the API request again. Then all these are just extracting what I want from from the data which is just the post. I don't want other things to get mixed in there. I just want the post data. Let's let's array in here. And I use this loop over item to make sure that only process one post at a time because every time I get a post link I actually use another file crawl method call scrape which is a lot quicker than extract because it's just a simple scrape of the content and then you can return it as a markdown. After we get the markdown from the page because you can see that if you click into any of the link in hacker news it's just a random article like it's not always on a white operator or a certain website. So it's always going to be different websites and different information and different format. So the the amount of text usually is is massive. So in order to not store too much data in our vector database I use summarizing chain. It summarized the data and afterwards that I finally store it into Panko. And in Panko I make sure that I store the content of the summarization and I also store the URL. The reason why I store the URL is that in the future I could add another step to check if I rescript this URL so I don't do it again and again if the website doesn't change that often. So that's the whole process of storing data from hacker news to vector database. And the other step is just setting up the agent. The agent you just need to use the AI agent node then give it a model, give it a memory. In production use you should probably use a postgres memory or something that can last longer. Windows buffer memory doesn't let you store it too much. And for the vector store it's also Panko because we were storing Panko earlier. So just make sure that it's always the same thing and also make sure that the embedding you're also using the same model. So that's pretty much it. You just need to set those up. Then you can have different use cases to this as well. You don't always have to make it into a chat bar. You can also have this thing running on the schedule where you just generate new content idea for you. And you just need to make sure that you run the work for the active vector database first. Then you run this so you have the fresh data as always. I hope that was helpful. Let me know if you have any questions. You can find me on LinkedIn as David you deaf.

--- Turn ANY Website into LLM Knowledge in SECONDS.txt ---
 One of the biggest challenges we face with large language models right now is their knowledge is too general and it's very limited for new things because of its training cutoff. For example, my favorite new shiny framework for building AI agents is PIDANTIC AI. But if I go over to Cloud right now and ask at what PIDANTIC AI is, it has no clue. And even if I use an LLM that can search the web, the information that I get back is still going to be very bare bones. But on the other hand, if I take all of the framework documentation for PIDANTIC AI and put it in a knowledge base for the LLM, and I ask at the exact same question, the answer that I get is spot on. That is why a rag is such a huge topic when it comes to AI right now, which by the way stands for retrieval augmented generation. It's a method for giving external knowledge that you curate yourself into an LLM, basically to make it an expert at something that it wasn't before. Like an AI agent framework you're using, your e-commerce store, you name it. The problem is that curate step can be very difficult and slow. For example, if you want to ingest an entire website into a knowledge base for your LLM, how do you actually do that and get it done fast so that it's not 2027 by the time your knowledge base is ready and AI has taken over the world anyway. That is where crawl for AI comes in. Crawl for AI is an open source web crawling framework, specifically designed to scrape websites and format the output in the best way for LLMs to understand. And the best part is, it solves a lot of problems that we typically see with these systems for website scraping. Usually they're very slow, overly complicated, and super resource intensive. But crawl for AI is the complete opposite. It's super intuitive, very, very fast, easy to set up, and extremely memory efficient. So in this video, I'll show you how to super easily use crawl for AI to scrape any website for an LLM in just seconds. And at the end of this video, I'll even quickly showcase a RIG AI agent that I built basically to be an expert at the PIDANTIC AI framework. Of course, using crawl for AI to curate all the framework knowledge into my knowledge base. And really, you could take what I built and use it for any website. Super exciting stuff. Let's go ahead and dive right into it. So the two big things that we're going to focus on right now is crawl for AI, which this is their GitHub right here, completely open source, and then also PIDANTIC AI. Now this is not a PIDANTIC AI video, but it's just a very good example of full documentation that we can scrape with crawl for AI and bring into a knowledge base for our LLM. Now back over to Crawl for AI. The first obvious question when we visit this page is, what is the point of crawl for AI? Why even use it? And luckily at the top of their readme here on their homepage, they have a very concise description for why you should care about crawl for AI. The first big thing is when you visit a website and you extract the raw HTML from it. I mean, this looks like a mess. And as a human looking at this, it's very hard for us to actually extract useful information from it. And a good general rule of thumb is if it's hard for us as a human to understand something, it's probably harder for a large language model as well. And so one of the most important things that Crawl for AI does is it takes this ugly HTML that we get when we visit the raw content of a web page and it turns it into a markdown format, which is actually even human readable format, like a super clean way to represent a web page and all the information that we see when we typically visit a UI like this, just in something that's all text space so we can give it to a large language model for Rags. And just for LLMs to understand it better in general. So that's the first thing. And it does it very, very efficiently. It's super, super fast. It handles a ton of things under the hood like proxies and session management, things that are not easy to handle. So it's not like you can go and make your own version of crawl for AI very easily as well, just because you know how to use the request module and Python to pull HTML from websites. It's also completely open source and very easy to deploy. They even have a Docker option, which I'll probably cover in another video on my channel later on. They're doing major updates soon to their Docker deployment. So I don't want to focus on that now, but just know that that is available as well. And there's a ton of other things that they do that are super valuable as well. Like one really important thing is removing irrelevant content. I mean, when we go to the HTML here, we have a ton of script tags and there's probably a lot of information that we can view on the page that is very obviously redundant or just not useful to us and that they take care of removing that as well. So eventually what we get back from scraping the site with crawl for AI contains just what we care about actually ingesting into our knowledge base. And getting started with crawl for AI is so easy. All you have to do is pip install the Python package and then run this setup command, which is going to install playwright under the hood. That is the tool that crawl for AI uses under the hood to scrape websites and basically have this browser running in the background that can visit these sites. And playwright is a fantastic, also open source tool that I use for a lot of testing for my web applications as well. So very, very familiar with it. I can recommend this as well. So I think it's a great choice for actually having that web scraping functionality under the hood and getting started is so, so easy. It's just as simple as this little script right here. And so I actually have my own version of it. If I go into my source control here, I've got my own version of it. Just scraping the homepage of patented AI, their documentation. So let's go ahead and test this out and see what kind of output we get. And by the way, all of the code that I go over in this video, I'll have any GitHub repository that I will link below. And so right now we're starting with just these basic examples right here, starting with this one to pull the homepage of the documentation for patented AI. And then we'll get to my rag AI agent later and I'll showcase that a little bit as well. And so yeah, I've already installed crawl for AI. It took like 30 seconds to install everything, including playwright under the hood. And so now I can just go ahead and run our script. And within seconds, we're going to have the entire page printed out in the terminal here. So, so fast. Very, very impressed with this. And this isn't like the perfect format for us as humans to understand because we have all this little markdown syntax and stuff. But this is definitely a great format for an LLM to understand this entire page, especially compared to what we get. If I just go back to the patented documentation here, I'll just open this back up. If I inspect the page source, this is the HTML that we got the markdown from. And imagine pasting all of this into an LLM prompt. I mean, it's just a mess. It's definitely going to hallucinate when you try to ask a question with all of the HTML tags and everything in there. And so it's just so much better when we have something that looks like this. All right. So we saw a basic example using crawl for AI to get the markdown for a single file. But obviously, we have to take this a lot further to do something that is actually useful for our LLMs. Now, the first thing that we want to do is make it possible to ingest every single one of the pages in the patented AI documentation. So you want to get the markdown for the introduction page, installation, getting help, contributing, all of these at the same time. And the first problem we have to tackle to actually make that possible is we need an efficient and scalable way to extract all of the URLs from the documentation here. Now I could just go and manually copy and paste the homepage and the installation page and the agent page and just bring that into a list in my code. But that is very inefficient and not scalable. As more pages are added, I'll have to manually do the same thing and constantly update the list myself. And so luckily, there is a very good solution for this. Using the idea of a site map. So for most pages out there on the internet right now, if you go to the homepage and then add slash site map.xml, it's going to give you this XML, which gives you the entire structure of the website, all of the pages that exist there. And so you can do this right now for the patented documentation like I'm doing right here. And all the pages that you see here are the same ones that we see in the navigation right here. And the same thing for the crawl for AI documentation. It's very meta, but I could use the site map.xml to get all the pages in the crawl for AI documentation to scrape with crawl for AI. So yeah, you can do this with most websites, most e-commerce stores that they're built with Shopify or WordPress. They'll have this as well. Because in general, websites have this for search engine optimization and also for crawlers. I mean, a lot of websites want you to crawl them because they want their information very widespread. Like if I'm building an AI agent framework, I want people to crawl this and build agents around it because then they're using my framework and it's just more accessible, which by the way, if you are curious, if you can scrape a website, there's a lot of ethics behind web scraping. Typically what you can do is go to a website like youtube.com and then add robots.text to the URL here. And this will give you a page that tells you their rules for web scraping. So this says that any agent is allowed to scrape youtube. However, there are certain pages that are not allowed. And so this is super important to keep in mind. If you want to be very ethical with your web scraping, which I highly recommend, check the websites you're scraping for robot.text first before you just go ahead and do it. Like GitHub is another good example here where they actually say if you want to crawl GitHub, please contact them first. A lot of them will be like this. So keep this in mind. I very much owe it to you to provide this little segment talking about ethics before I dive into the rest of the video. It is very fitting to do that because we're talking about URLs that you can add to pretty much any websites. You can do slash robots.text or the slash site map dot XML. And so we're going to encode pull this site map and then we're going to extract every single URL and then feed all of those into crawl for AI. And we want to do that very efficiently as well because right here we're just going to be pulling in every URL and then looping and going through one at a time. If we do it just in a loop with this code right here, we want something more efficient. And so if we go to the documentation for crawl for AI, which is right here, that'll bring us to this page right here. And if we go down to multi URL crawling, there is a lot that crawl for AI gives us for this. And that's what we're going to be leveraging for the rest of this video. So first of all, if you just crawl your websites in a loop like this, like we would do if we just continued off of this example right here, it would be very inefficient. We're spinning up a brand new browser for every URL that we are visiting. And there's no opportunity for parallel processing, which we're going to get into as well. And so their recommendation, they give a full example for this for how you can use the same browser session for all of the pages that you're visiting and pulling. And so that brings us to the second script that I have built for us here. And I'm not going to go over all the code in detail because this is mostly following the example that we just saw right here. So I just copied this in, brought it into my code editor, and then I have this custom function right here where I pull that site map that I just showed you. So I pull it, I extract using XML processing, all the URLs from the site map. And then I pass them all into this function to crawl the URLs sequentially with the same browser session. And so the code gets a little complicated with the browser config and crawler config. But don't worry about that. In general, you can just take this example and use it for yourself. It crawls every single URL. And it's not going to print out the content of every markdown because that would just be way too much in the terminal. It would just show us the length and whether or not it succeeded in crawling the site. And so I'm going to go back to my terminal here and run this second script here. And it's going to take a little bit because I have to crawl all of them sequentially. We'll get into parallel processing next to make this even faster. But even this just took seconds. It was so fast processing each one of these pages, giving me the length and whether it's a success or not for each one of these URLs. So at this point, we already have a very fast way to get the markdown for every single Panantik AI documentation page. And it's ready now for us to put in a vector database for rag to use with our large language model. It's super neat. And it was so easy to set this up with crawl for AI. But before we actually get into anything with rag, I want to take this one step further. Because I want to make this even faster. It was already fast here. But we're still processing each one of these URLs sequentially. There's no parallel processing. And we can definitely do that with crawl for AI. So we can visit multiple pages at the exact same time, pull the markdown for every single one of them, and then combine it all at the end to a single list just like we're doing right here. And so the way that we can do that, if I go back to the crawl for AI documentation and just scroll down a little bit, they have an example doing this exactly, this parallel processing. And it's essentially going to be the same. We're still just using one browser. But we're creating different sessions that are all going to be up at the same time visiting these URLs in parallel. And just like last time, I mostly just copied the example that they had right here and then brought it into my code editor. And then again, just like last time, the main thing that I added is that function to use the pedantic AI site map.xml to get all the pages that I want to pull the markdown for and scrape. And so I'm going to open up my terminal again. And actually one last thing before I do that for the batch size we are doing 10. So it's going to visit 10 pages at the exact same time, get the markdown for all of them and then move on to the next set of 10 and then repeat that until it is pulled the markdown for every single page. And so I have a new terminal open up right here. I'm just going to run this script just like I did before. And last time I showed how fast each run was, now I'm going to show how memory efficient it is. When I run this, it will show the current RAM usage for the script, which it starts at 91 megabytes. It's going through all these batches very, very quickly. At the end, we can see the peak usage, which is only 119 megabytes. So throughout this entire time, even though there's an entire browser running in the background visiting 10 pages at a time, it's still only ever used 119 megabytes of memory at once, which is just incredible. And the last example was actually basically as fast, but that's only because of caching. In general, this batch processing is going to speed it up a ton, which is super impressive. And so now we have the perfect thing for reg because we're doing it very, very quickly. And a lot of times you need that. I mean, this example has 42 pages. But if you have something like an e-commerce store with hundreds or thousands of products, you can imagine that this is going to start to be a drag if you are processing things sequentially, not using the same browser in same session. That's why using crawl for AI and having all these efficiencies is so important. Very last thing, and this is my true gift to you. I have already built out a full reg AI agent that is a pedantic AI expert. So using the exact same process with crawl for AI that we just did, I pulled all of the pedantic AI documentation. And then I put it into a vector database for its knowledge base, built a full agent around it, and created a front end that we're looking at right here. And my gift to you is this is already available to you. I have the code in a GitHub repository that I have linked below. And then in the next video on my channel, I'll be covering how I actually built this agent. And it will be available on the live Asian studio for you to try immediately. So super, super neat. And also a little bit of a sneak peek right now. So let me paste this in here. I'll just ask a basic question like what are the supported models? And this is the kind of thing that cloud or any other general LLM would definitely not have the answer for. And it even links me to the different pages in the pedantic AI documentation for my reference, very, very neat. And I can ask a ton of other questions as well. Like I can say something like give me the weather agent example from the documentation. Obviously, I just know that this agent example exists. And so I'll ask for it. And it'll go in search for it. Find this full example for me. And it does it so fast as well. And this is perfect. Like this is a pretty complex example because it's showing me basically every part of creating an agent with pedantic AI, which is super, super neat. So there we go. This is the full agent. And I'll be showing you exactly how to build it very, very soon on my channel. Now the reason that I'm not covering how to build the entire agent in this video is I want to keep it concise and focused on just crawl for AI, especially because there's a lot of other use cases for web scraping besides just rag. Even though rag is definitely one of the biggest ones for AI and just in general right now. But if you go to the GitHub repo, I have a read me right here covering everything with this agent. You have all the code for my entire process of crawling all these sites again with a very similar process to what we went over in this video. And then actually inserting that into our vector database, which I'm actually using PG vector with super base here. And then I have my agent that I built with pedantic AI, very meta, but it is my favorite framework right now. And then I have my stream layer interface. So all this is available for you with instructions on how to run it yourself. And then also stay tuned for my video later this week where I'll show you exactly how I built it. So there you have it, a bulletproof lightning fast way to scrape any site and give it to your LLM as a knowledge base. And this is useful for you pretty much no matter what your use case is because there's almost always a time and place to take data from external websites and bring that into your LLM. And so that in my mind makes crawl for AI a game changer. And don't get me wrong, there are a lot of ways to bring knowledge into an LLM. You can manually curate data, use new advanced concepts like Kagg, a lot of things I'll cover in more videos on my channel. But it is still the most common way to make an AI agent an expert, something you care about to scrape data from a site and provide it to a knowledge base for it. And in the next video on my channel, I'll do a deep dive into the rag AI agent that I download earlier, which I am super excited about because I've put a lot of effort into building it for you. So if you appreciate this content, I would really appreciate a like and a subscribe. And with that, I will see you in the next video.
